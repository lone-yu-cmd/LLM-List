{
  "path": "/chat/completions",
  "method": "POST",
  "description": "Creates a model response for the given chat conversation.",
  "request_parameters": {
    "type": "object",
    "format": "json_object",
    "properties": {
      "messages": {
        "type": "array",
        "format": "list",
        "description": "A list of messages comprising the conversation so far.",
        "items": {
          "type": "object",
          "format": "json_object",
          "properties": {
            "role": {
              "type": "string",
              "format": "string",
              "enum": ["system", "user", "assistant", "tool", "function"],
              "description": "The role of the messages author."
            },
            "content": {
              "type": "string",
              "format": "string",
              "description": "The contents of the message."
            },
            "name": {
              "type": "string",
              "format": "string",
              "description": "An optional name for the participant. Provides the model information to differentiate between participants of the same role."
            }
          },
          "required": ["role", "content"]
        }
      },
      "model": {
        "type": "string",
        "format": "string",
        "description": "ID of the model to use."
      },
      "frequency_penalty": {
        "type": "number",
        "format": "float",
        "default": 0,
        "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far."
      },
      "logit_bias": {
        "type": "object",
        "format": "map",
        "description": "Modify the likelihood of specified tokens appearing in the completion."
      },
      "max_tokens": {
        "type": "integer",
        "format": "int32",
        "description": "The maximum number of tokens to generate in the chat completion."
      },
      "n": {
        "type": "integer",
        "format": "int32",
        "default": 1,
        "description": "How many chat completion choices to generate for each input message."
      },
      "presence_penalty": {
        "type": "number",
        "format": "float",
        "default": 0,
        "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far."
      },
      "response_format": {
        "type": "object",
        "format": "json_object",
        "description": "An object specifying the format that the model must output."
      },
      "seed": {
        "type": "integer",
        "format": "int64",
        "description": "This feature is in Beta. If specified, our system will make a best effort to sample deterministically."
      },
      "stop": {
        "type": "string",
        "format": "string",
        "description": "Up to 4 sequences where the API will stop generating further tokens."
      },
      "stream": {
        "type": "boolean",
        "format": "boolean",
        "default": false,
        "description": "If set, partial message deltas will be sent, like in ChatGPT."
      },
      "temperature": {
        "type": "number",
        "format": "float",
        "default": 1,
        "description": "What sampling temperature to use, between 0 and 2."
      },
      "top_p": {
        "type": "number",
        "format": "float",
        "default": 1,
        "description": "An alternative to sampling with temperature, called nucleus sampling."
      },
      "tools": {
        "type": "array",
        "format": "list",
        "description": "A list of tools the model may call."
      },
      "tool_choice": {
        "type": "string",
        "format": "string",
        "description": "Controls which (if any) tool is called by the model."
      },
      "user": {
        "type": "string",
        "format": "string",
        "description": "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse."
      }
    },
    "required": ["messages", "model"]
  },
  "response_parameters": {
    "type": "object",
    "format": "json_object",
    "properties": {
      "id": {
        "type": "string",
        "format": "string",
        "description": "A unique identifier for the chat completion."
      },
      "choices": {
        "type": "array",
        "format": "list",
        "description": "A list of chat completion choices.",
        "items": {
          "type": "object",
          "format": "json_object",
          "properties": {
            "index": {
              "type": "integer",
              "format": "int32",
              "description": "The index of the choice in the list of choices."
            },
            "message": {
              "type": "object",
              "format": "json_object",
              "description": "A chat completion message generated by the model."
            },
            "finish_reason": {
              "type": "string",
              "format": "string",
              "description": "The reason the model stopped generating tokens."
            }
          }
        }
      },
      "created": {
        "type": "integer",
        "format": "int64",
        "description": "The Unix timestamp (in seconds) of when the chat completion was created."
      },
      "model": {
        "type": "string",
        "format": "string",
        "description": "The model used for the chat completion."
      },
      "system_fingerprint": {
        "type": "string",
        "format": "string",
        "description": "This fingerprint represents the backend configuration that the model runs with."
      },
      "object": {
        "type": "string",
        "format": "string",
        "description": "The object type, which is always chat.completion."
      },
      "usage": {
        "type": "object",
        "format": "json_object",
        "properties": {
          "completion_tokens": {
            "type": "integer",
            "format": "int32",
            "description": "Number of tokens in the generated completion."
          },
          "prompt_tokens": {
            "type": "integer",
            "format": "int32",
            "description": "Number of tokens in the prompt."
          },
          "total_tokens": {
            "type": "integer",
            "format": "int32",
            "description": "Total number of tokens used in the request (prompt + completion)."
          }
        }
      }
    }
  }
}
