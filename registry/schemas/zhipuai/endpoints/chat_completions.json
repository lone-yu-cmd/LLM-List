{
  "path": "/chat/completions",
  "docs_url": "https://open.bigmodel.cn/dev/api/normal-model/glm-4#chat-completions",
  "description": "Synchronous call, wait for the model to complete execution and return the final result or use SSE call.",
  "method": "POST",
  "request_content_type": "application/json",
  "response_content_type": ["application/json", "text/event-stream"],
  "character_encoding": "UTF-8",
  "supports_stream": true,
  "request_parameters": {
    "model": {
      "type": "string",
      "format": "string",
      "required": true,
      "description": "The ID of the model to use (e.g., 'glm-4-plus')."
    },
    "messages": {
      "type": "array",
      "format": "array",
      "required": true,
      "description": "A list of messages comprising the conversation so far.",
      "items": {
        "type": "object",
        "format": "object",
        "properties": {
          "role": {
            "type": "string",
            "format": "string",
            "enum": ["system", "user", "assistant", "tool"],
            "description": "The role of the messages author."
          },
          "content": {
            "type": "string",
            "format": "string",
            "description": "The contents of the message."
          },
          "tool_calls": {
            "type": "array",
            "format": "array",
            "description": "The tool calls generated by the model, such as function calls."
          }
        },
        "required": ["role", "content"]
      }
    },
    "request_id": {
      "type": "string",
      "format": "string",
      "required": false,
      "description": "Unique identifier for the request, generated by the client."
    },
    "do_sample": {
      "type": "boolean",
      "format": "boolean",
      "required": false,
      "default": true,
      "description": "Whether to use sampling; use false for deterministic results."
    },
    "stream": {
      "type": "boolean",
      "format": "boolean",
      "required": false,
      "default": false,
      "description": "If true, partial message deltas will be sent as data-only server-sent events."
    },
    "temperature": {
      "type": "number",
      "format": "float",
      "required": false,
      "default": 0.95,
      "minimum": 0.0,
      "maximum": 1.0,
      "description": "Sampling temperature. Higher values mean more random outputs."
    },
    "top_p": {
      "type": "number",
      "format": "float",
      "required": false,
      "default": 0.7,
      "minimum": 0.0,
      "maximum": 1.0,
      "description": "Nucleus sampling. Model considers the results of the tokens with top_p probability mass."
    },
    "max_tokens": {
      "type": "integer",
      "format": "int32",
      "required": false,
      "description": "The maximum number of tokens to generate in the completion."
    },
    "stop": {
      "type": "array",
      "format": "array",
      "required": false,
      "items": {
        "type": "string",
        "format": "string"
      },
      "description": "Up to 4 sequences where the API will stop generating further tokens."
    },
    "tools": {
      "type": "array",
      "format": "array",
      "required": false,
      "description": "A list of tools the model may call. Currently, only functions are supported as a tool.",
      "items": {
        "type": "object",
        "format": "object",
        "properties": {
          "type": {
            "type": "string",
            "format": "string",
            "enum": ["function", "retrieval", "web_search"],
            "description": "The type of the tool."
          },
          "function": {
            "type": "object",
            "format": "object",
            "description": "Function definition."
          },
          "retrieval": {
            "type": "object",
            "format": "object",
            "description": "Retrieval tool configuration."
          },
          "web_search": {
            "type": "object",
            "format": "object",
            "description": "Web search tool configuration."
          }
        }
      }
    },
    "tool_choice": {
      "type": "string",
      "format": "string",
      "required": false,
      "default": "auto",
      "enum": ["auto", "none"],
      "description": "Controls which (if any) tool is called by the model."
    },
    "user_id": {
      "type": "string",
      "format": "string",
      "required": false,
      "description": "A unique identifier representing your end-user."
    }
  },
  "response_parameters": {
    "id": {
      "type": "string",
      "format": "string",
      "description": "A unique identifier for the chat completion."
    },
    "created": {
      "type": "integer",
      "format": "int64",
      "description": "The Unix timestamp (in seconds) of when the chat completion was created."
    },
    "model": {
      "type": "string",
      "format": "string",
      "description": "The model used for the chat completion."
    },
    "choices": {
      "type": "array",
      "format": "array",
      "description": "A list of chat completion choices.",
      "items": {
        "type": "object",
        "format": "object",
        "properties": {
          "index": {
            "type": "integer",
            "format": "int32",
            "description": "The index of the choice in the list of choices."
          },
          "message": {
            "type": "object",
            "format": "object",
            "description": "The chat completion message generated by the model.",
            "properties": {
              "role": {
                "type": "string",
                "format": "string",
                "description": "The role of the author of this message."
              },
              "content": {
                "type": "string",
                "format": "string",
                "description": "The contents of the message."
              },
              "tool_calls": {
                "type": "array",
                "format": "array",
                "description": "The tool calls generated by the model, such as function calls."
              }
            }
          },
          "finish_reason": {
            "type": "string",
            "format": "string",
            "description": "The reason the model stopped generating tokens."
          }
        }
      }
    },
    "usage": {
      "type": "object",
      "format": "object",
      "description": "Usage statistics for the completion request.",
      "properties": {
        "prompt_tokens": {
          "type": "integer",
          "format": "int32",
          "description": "Number of tokens in the prompt."
        },
        "completion_tokens": {
          "type": "integer",
          "format": "int32",
          "description": "Number of tokens in the generated completion."
        },
        "total_tokens": {
          "type": "integer",
          "format": "int32",
          "description": "Total number of tokens used in the request (prompt + completion)."
        }
      }
    }
  }
}
